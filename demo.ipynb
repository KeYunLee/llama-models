{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 掛載google drive"],"metadata":{"id":"Otbl2sCtd2eq"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%matplotlib inline\n","\n","import os\n","from glob import glob\n","from tqdm import tqdm"],"metadata":{"id":"YLfqTyMj0nml","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727412466820,"user_tz":-480,"elapsed":31153,"user":{"displayName":"李克耘","userId":"07893560853012048440"}},"outputId":"d65afa3a-8f68-4d8b-ccea-a296ac85592c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# 安裝llama套件"],"metadata":{"id":"MCqhrcwOeMu5"}},{"cell_type":"code","source":["# Option - I: Uncomment the next line if you want to install via pip.\n","!pip install llama-stack"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRDy_qbO0uEA","executionInfo":{"status":"ok","timestamp":1727412506386,"user_tz":-480,"elapsed":7429,"user":{"displayName":"李克耘","userId":"07893560853012048440"}},"outputId":"574d69c4-8316-42cc-e5a9-3f3da26f4922"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-stack\n","  Downloading llama_stack-0.0.36-py3-none-any.whl.metadata (5.2 kB)\n","Collecting blobfile (from llama-stack)\n","  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n","Collecting fire (from llama-stack)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx (from llama-stack)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from llama-stack) (0.24.7)\n","Collecting llama-models>=0.0.36 (from llama-stack)\n","  Downloading llama_models-0.0.36-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: prompt-toolkit in /usr/local/lib/python3.10/dist-packages (from llama-stack) (3.0.47)\n","Collecting python-dotenv (from llama-stack)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llama-stack) (2.9.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from llama-stack) (2.32.3)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from llama-stack) (13.8.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from llama-stack) (2.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack) (6.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack) (3.1.4)\n","Collecting tiktoken (from llama-models>=0.0.36->llama-stack)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from llama-models>=0.0.36->llama-stack) (10.4.0)\n","Collecting pycryptodomex>=3.8 (from blobfile->llama-stack)\n","  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile->llama-stack) (2.2.3)\n","Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->llama-stack) (4.9.4)\n","Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile->llama-stack) (3.16.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llama-stack) (1.16.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-stack) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-stack) (2024.8.30)\n","Collecting httpcore==1.* (from httpx->llama-stack)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-stack) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-stack) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-stack)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack) (24.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->llama-stack) (4.12.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit->llama-stack) (0.2.13)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llama-stack) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->llama-stack) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->llama-stack) (3.3.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->llama-stack) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->llama-stack) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-stack) (1.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->llama-models>=0.0.36->llama-stack) (2.1.5)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-models>=0.0.36->llama-stack) (2024.9.11)\n","Downloading llama_stack-0.0.36-py3-none-any.whl (223 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading llama_models-0.0.36-py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=64d3fcccea3d0b40a79f2355ad9bf2c7a30e991ec1e87873cb7d812d51567e01\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","Successfully built fire\n","Installing collected packages: python-dotenv, pycryptodomex, h11, fire, tiktoken, httpcore, blobfile, llama-models, httpx, llama-stack\n","Successfully installed blobfile-3.0.0 fire-0.6.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 llama-models-0.0.36 llama-stack-0.0.36 pycryptodomex-3.20.0 python-dotenv-1.0.1 tiktoken-0.7.0\n"]}]},{"cell_type":"markdown","source":["# Find models list"],"metadata":{"id":"p7kw1wY7eP64"}},{"cell_type":"code","source":["!llama model list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KnEtdMi0Aj3","executionInfo":{"status":"ok","timestamp":1727412594675,"user_tz":-480,"elapsed":1064,"user":{"displayName":"李克耘","userId":"07893560853012048440"}},"outputId":"027e1030-c143-4bf0-964e-2e19f8d62518"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------+------------------------------------------+----------------+\n","\u001b[1m\u001b[97m| Model Descriptor                 | HuggingFace Repo                         | Context Length |\u001b[0m\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-8B                      | meta-llama/Llama-3.1-8B                  | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-70B                     | meta-llama/Llama-3.1-70B                 | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B:bf16-mp8           | meta-llama/Llama-3.1-405B                | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B                    | meta-llama/Llama-3.1-405B-FP8            | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B:bf16-mp16          | meta-llama/Llama-3.1-405B                | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-8B-Instruct             | meta-llama/Llama-3.1-8B-Instruct         | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-70B-Instruct            | meta-llama/Llama-3.1-70B-Instruct        | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B-Instruct:bf16-mp8  | meta-llama/Llama-3.1-405B-Instruct       | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B-Instruct           | meta-llama/Llama-3.1-405B-Instruct-FP8   | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.1-405B-Instruct:bf16-mp16 | meta-llama/Llama-3.1-405B-Instruct       | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-1B                      | meta-llama/Llama-3.2-1B                  | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-3B                      | meta-llama/Llama-3.2-3B                  | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-11B-Vision              | meta-llama/Llama-3.2-11B-Vision          | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-90B-Vision              | meta-llama/Llama-3.2-90B-Vision          | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-1B-Instruct             | meta-llama/Llama-3.2-1B-Instruct         | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-3B-Instruct             | meta-llama/Llama-3.2-3B-Instruct         | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-11B-Vision-Instruct     | meta-llama/Llama-3.2-11B-Vision-Instruct | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama3.2-90B-Vision-Instruct     | meta-llama/Llama-3.2-90B-Vision-Instruct | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-3-11B-Vision         | meta-llama/Llama-Guard-3-11B-Vision      | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-3-1B:int4-mp1        | meta-llama/Llama-Guard-3-1B-INT4         | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-3-1B                 | meta-llama/Llama-Guard-3-1B              | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-3-8B                 | meta-llama/Llama-Guard-3-8B              | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-3-8B:int8-mp1        | meta-llama/Llama-Guard-3-8B-INT8         | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Prompt-Guard-86M                 | meta-llama/Prompt-Guard-86M              | 128K           |\n","+----------------------------------+------------------------------------------+----------------+\n","| Llama-Guard-2-8B                 | meta-llama/Llama-Guard-2-8B              | 4K             |\n","+----------------------------------+------------------------------------------+----------------+\n"]}]},{"cell_type":"code","source":["!llama model download --source meta --model-id Llama3.2-3B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05CEWiUM0GAz","executionInfo":{"status":"ok","timestamp":1727412746971,"user_tz":-480,"elapsed":66619,"user":{"displayName":"李克耘","userId":"07893560853012048440"}},"outputId":"91c7ad2e-f94f-48ad-a208-71935cb03479"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Please provide the signed URL you received via email (e.g., https://llama3-1.llamameta.net/*?Policy...): \n","^C\n"]}]},{"cell_type":"code","source":["!pip install llama-toolchain"],"metadata":{"id":"8Ua82tiH0bQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch fairscale fire blobfile"],"metadata":{"id":"d0fNY3Dj2CZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash ./demo.sh"],"metadata":{"id":"PkH82QZT4ErJ"},"execution_count":null,"outputs":[]}]}